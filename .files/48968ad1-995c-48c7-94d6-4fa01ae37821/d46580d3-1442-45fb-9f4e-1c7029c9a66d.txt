LLaMA: open and
efficient foundation language models. CoRR , abs/2302.13971, 2023.
[TMS+23]Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan
Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David
Esiobu, Jude Fernandes, Jeremy Fu, and et al. Llama 2: open foundation and fine-tuned
chat models. CoRR , abs/2307.09288, 2023.
[WLG17] Johannes Welbl, Nelson F. Liu, and Matt Gardner. Crowdsourcing multiple choice
science questions. In Leon Derczynski, Wei Xu, Alan Ritter, and Tim Baldwin, editors,
Proceedings of the 3rd Workshop on Noisy User-generated Text, NUT@EMNLP 2017,
Copenhagen, Denmark, September 7, 2017 , pages 94â€“106. Association for Computa-
tional Linguistics, 2017.
[WMC+23]Lei Wang, Lingxiao Ma, Shijie Cao, Ningxin Zheng, Quanlu Zhang, Jilong Xue, Ziming
Miao, Ting Cao, , and Yuqing Yang. Ladder: Efficient tensor compilation on customized
data format. In OSDI , 2023.
[WMD+23]Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma,
Fan Yang, Ruiping Wang, Yi Wu, and Furu Wei. Bitnet: Scaling 1-bit transformers for
large language models. CoRR , abs/2310.11453, 2023.
7