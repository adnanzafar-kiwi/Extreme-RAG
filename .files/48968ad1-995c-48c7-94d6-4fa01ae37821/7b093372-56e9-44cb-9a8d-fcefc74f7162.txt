[LTT+23]Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. AWQ:
activation-aware weight quantization for LLM compression and acceleration. CoRR ,
abs/2306.00978, 2023.
[MCKS18] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of
armor conduct electricity? A new dataset for open book question answering. CoRR ,
abs/1809.02789, 2018.
[MXBS16] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel
mixture models, 2016.
[PKL+16]Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella
Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The
LAMBADA dataset: Word prediction requiring a broad discourse context. In Proceed-
ings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL
2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers . The Association
for Computer Linguistics, 2016.
[RSR+19]Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning
with a unified text-to-text transformer. CoRR , abs/1910.10683, 2019.
[SAL+24]Jianlin Su, Murtadha H. M. Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu.
Roformer: Enhanced transformer with rotary position embedding. Neurocomputing ,
568:127063, 2024.
[SBBC20] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Wino-
Grande: an adversarial winograd schema challenge at scale. In The Thirty-Fourth AAAI
Conference on Artificial Intelligence , pages 8732–8740, 2020.
[Sha20] Noam Shazeer. GLU variants improve transformer. CoRR , abs/2002.05202, 2020.
[TBMR] Jonathan Tow, Marco Bellagente, Dakota Mahan, and Carlos Riquelme. Stablelm 3b
4e1t.
[TCS+24]Albert Tseng, Jerry Chee, Qingyao Sun, V olodymyr Kuleshov, and Christopher De
Sa. Quip#: Even better LLM quantization with hadamard incoherence and lattice
codebooks. CoRR , abs/2402.04396, 2024.
[TLI+23]Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,
Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien
Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: open and
efficient foundation language models. CoRR , abs/2302.13971, 2023.
[TMS+23]Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan
Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David
Esiobu, Jude Fernandes, Jeremy Fu, and et al. Llama 2: open foundation and fine-tuned
chat models. CoRR , abs/2307.09288, 2023.
[WLG17] Johannes Welbl, Nelson F. Liu, and Matt Gardner. Crowdsourcing multiple choice
science questions.