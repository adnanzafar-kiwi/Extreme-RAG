BitNet b1.58 LLaMA0.00.10.20.30.40.57nm Energy Cost (pJ)71.4xINT8 Add
FP16 Add
FP16 Mul
1.3B 3B 7B 13B 70B
Model Size101
100101Energy (J)
18.6x21.7x29.1x32.9x41.2xBitNet b1.58
LLaMAFigure 3: Energy consumption of BitNet b1.58 compared to LLaMA LLM at 7nm process nodes. On
the left is the components of arithmetic operations energy. On the right is the end-to-end energy cost
across different model sizes.
Models Tokens Winogrande PIQA SciQ LAMBADA ARC-easy Avg.
StableLM-3B 2T 64.56 76.93 90.75 66.09 67.78 73.22
BitNet b1.58 3B 2T 66.37 78.40 91.20 67.63 68.12 74.34
Table 4: Comparison of BitNet b1.58 with StableLM-3B with 2T tokens.
BitNet b1.58 is enabling a new scaling law with respect to model performance and inference
cost. As a reference, we can have the following equivalence between different model sizes in 1.58-bit
and 16-bit based on the results in Figure 2 and 3.
•13B BitNet b1.58 is more efficient, in terms of latency, memory usage and energy consump-
tion, than 3B FP16 LLM.
•30B BitNet b1.58 is more efficient, in terms of latency, memory usage and energy consump-
tion, than 7B FP16 LLM.
•70B BitNet b1.58 is more efficient, in terms of latency, memory usage and energy consump-
tion, than 13B FP16 LLM.
Training with 2T Tokens The number of training tokens is a crucial factor for LLMs. To test
the scalability of BitNet b1.58 in terms of tokens, we trained a BitNet b1.58 model with 2T to-
kens following the data recipe of StableLM-3B [ TBMR ], which is the state-of-the-art open-source
3B model. Both models were evaluated on a benchmark that consists of Winogrande [ SBBC20 ],
PIQA [ BZB+19], SciQ [ WLG17 ], LAMBADA [ PKL+16], and ARC-easy [ YBS19 ]. We reported
the zero-shot accuracy in Table 4. For tasks measured with accuracy and normalized accuracy, we
take the average of the two. The results of StableLM 3b at 2T tokens are taken directly from its
technical report. Our findings shows that BitNet b1.58 achieves a superior performance on all end
tasks, indicating that 1.58-bit LLMs also have strong generalization capabilities.
4 Discussion and Future Work
1-bit Mixture-of-Experts (MoE) LLMs
Mixture-of-Experts (MoE) have proven to be a cost-effective approach for LLMs. While it signifi-
cantly reduces the computation FLOPs, the high memory consumption and inter-chip communication
overhead limit its deployment and application. These challenges can be addressed by 1.58-bit LLMs.
Firstly, the reduced memory footprint reduces the number of devices required to deploy MoE models.
Moreover, it significantly reduces the overhead of transferring activations across networks. Ultimately,
there would be no overhead if the entire models could be placed on a single chip.
5