Native Support of Long Sequence in LLMs
In the era of LLMs, the ability to handle long sequence has become a critical demand. One major
challenge for long sequence inference is the memory consumption introduced by the KV caches.
BitNet b1.58 represents a significant step towards native support for long sequences, as it reduces the
activations from 16 bits to 8 bits, allowing the context length to be doubled given the same resources.
This can be further losslessly compressed to 4 bits or even lower for 1.58-bit LLMs, which we leave
as future work.
LLMs on Edge and Mobile
The use of 1.58-bit LLMs has the potential to greatly improve the performance of language models
on edge and mobile devices. These devices are often limited by their memory and computational
power, which can restrict the performance and the scale of LLMs. However, the reduced memory and
energy consumption of 1.58-bit LLMs allows them to be deployed on these devices, enabling a wide
range of applications that were previously not possible. This can greatly enhance the capabilities
of edge and mobile devices and enable new and exciting applications of LLMs. Moreover, 1.58-bit
LLMs are more friendly to CPU devices, which are the main processors used in edge and mobile
devices. This means that BitNet b1.58 can be efficiently executed on these devices, further improving
their performance and capabilities.
New Hardware for 1-bit LLMs
Recent work like Groq5has demonstrated promising results and great potential for building specific
hardware (e.g., LPUs) for LLMs. Going one step further, we envision and call for actions to design
new hardware and system specifically optimized for 1-bit LLMs, given the new computation paradigm
enabled in BitNet [WMD+23].
References
[BZB+19]Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA:
reasoning about physical commonsense in natural language. CoRR , abs/1911.11641,
2019.
[CCKS23] Jerry Chee, Yaohui Cai, V olodymyr Kuleshov, and Christopher De Sa. QuIP: 2-bit
quantization of large language models with guarantees. CoRR , abs/2307.13304, 2023.
[CLC+19]Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins,
and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no
questions. CoRR , abs/1905.10044, 2019.
[Com23] Together Computer. Redpajama: an open dataset for training large language models,
2023.
[FAHA23] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. OPTQ: accurate
quantization for generative pre-trained transformers. In The Eleventh International
Conference on Learning Representations , 2023.
[HCB+19]Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Xu Chen,
HyoukJoong Lee, Jiquan Ngiam, Quoc V . Le, Yonghui Wu, and Zhifeng Chen. Gpipe:
Efficient training of giant neural networks using pipeline parallelism. In Advances in
Neural Information Processing Systems , pages 103–112, 2019.
[Hor14] Mark Horowitz. 1.1 computing’s energy problem (and what we can do about it). In 2014
IEEE International Conference on Solid-State Circuits Conference, ISSCC 2014, Digest
of Technical Papers, San Francisco, CA, USA, February 9-13, 2014 , pages 10–14, 2014.
[KLZ+23]Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao
Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for
large language model serving with pagedattention. In Proceedings of the ACM SIGOPS
29th Symposium on Operating Systems Principles , 2023.
5https://groq.com/
6