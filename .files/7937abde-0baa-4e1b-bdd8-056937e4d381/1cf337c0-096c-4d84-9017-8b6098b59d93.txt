1.3B 3B 7B 13B 70B
Model Size100101102Latency (ms)
1.67x2.71x2.90x3.68x4.10xBitNet b1.58
LLaMA
1.3B 3B 7B 13B 70B
Model Size100101102Memory (GB)
2.93x3.55x4.40x5.12x7.16xBitNet b1.58
LLaMAFigure 2: Decoding latency (Left) and memory consumption (Right) of BitNet b1.58 varying the
model size.
Models Size Max Batch Size Throughput (tokens/s)
LLaMA LLM 70B 16 (1.0x) 333 (1.0x)
BitNet b1.58 70B 176 (11.0x) 2977 (8.9x)
Table 3: Comparison of the throughput between BitNet b1.58 70B and LLaMA LLM 70B.
Table 2 reports the detailed results of the zero-shot accuracy on the end tasks. We followed the pipeline
from lm-evaluation-harness4to perform the evaluation. The results show that the performance gap
between BitNet b1.58 andLLaMA LLM narrows as the model size increases. More importantly,
BitNet b1.58 can match the performance of the full precision baseline starting from a 3B size. Similar
to the observation of the perplexity, the end-task results reveal that BitNet b1.58 3.9B outperforms
LLaMA LLM 3B with lower memory and latency cost. This demonstrates that BitNet b1.58 is a
Pareto improvement over the state-of-the-art LLM models.
Memory and Latency We further scaled up the model size to 7B, 13B, and 70B and evaluated the
cost. Figure 2 illustrates the trends of latency and memory, showing that the speed-up increases as the
model size scales. In particular, BitNet b1.58 70B is 4.1 times faster than the LLaMA LLM baseline.
This is because the time cost for nn.Linear grows with the model size. The memory consumption
follows a similar trend, as the embedding remains full precision and its memory proportion is smaller
for larger models. Both latency and memory were measured with a 2-bit kernel, so there is still room
for optimization to further reduce the cost.
Energy We also estimate the arithmetic operations energy consumption of both BitNet b1.58 and
LLaMA LLM . We focus mainly on the calculation for matrix multiplication, since it contributes
the most to the cost of LLMs. Figure 3 illustrates the composition of the energy cost. The majority
ofBitNet b1.58 is INT8 addition calculation, while LLaMA LLM consists of both FP16 addition
and FP16 multiplication. According to the energy model in [ Hor14 ,ZZL22 ],BitNet b1.58 saves
71.4 times arithmetic operations energy consumption for matrix multiplication on 7nm chips. We
further reported the end-to-end energy cost for models with 512 tokens. Our results show that as the
model size scales, BitNet b1.58 becomes increasingly more efficient in terms of energy consumption
compared to the FP16 LLaMA LLM baseline. This is due to the fact that the percentage of nn.Linear
grows with the model size, while the cost from other components is smaller for larger models.
Throughput We compare the throughput of BitNet b1.58 andLLaMA LLM with 70B parameters
on two 80GB A100 cards, using pipeline parallelism [ HCB+19] so that LLaMA LLM 70B could be
run on the devices. We increased the batch size until the GPU memory limit was reached, with a
sequence length of 512. Table 3 shows that BitNet b1.58 70B can support up to 11 times the batch
size of LLaMA LLM, resulting an 8.9 times higher throughput.
4https://github.com/EleutherAI/lm-evaluation-harness
4